#!/usr/bin/env python3
"""
Comprehensive Test Runner for Enhanced Skill Development System
Tasks 3.2.1.1, 3.2.6.1, and 3.2.7.2

This script runs all test suites for the enhanced skill development system,
including discovery, optimization, and integration tests.
"""

import sys
import os
import unittest
import time
from datetime import datetime

# Add the parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import test modules
from test_skill_discovery_system import TestDynamicSkillDiscovery, TestSkillDiscoveryPerformance
from test_skill_optimization import TestSkillOptimization, TestSkillOptimizationPerformance
from test_skill_integration import TestSkillSystemIntegration, TestSkillSystemRegressionTests


def run_test_suite(suite_name: str, test_classes: list, verbose: bool = True) -> dict:
    """Run a test suite and return results"""
    print(f"\n{'='*60}")
    print(f"Running {suite_name}")
    print(f"{'='*60}")
    
    # Create test suite
    suite = unittest.TestSuite()
    for test_class in test_classes:
        suite.addTest(unittest.makeSuite(test_class))
    
    # Run tests
    if verbose:
        runner = unittest.TextTestRunner(verbosity=2, stream=sys.stdout)
    else:
        runner = unittest.TextTestRunner(verbosity=1, stream=sys.stdout)
    
    start_time = time.time()
    result = runner.run(suite)
    duration = time.time() - start_time
    
    # Calculate results
    total_tests = result.testsRun
    failures = len(result.failures)
    errors = len(result.errors)
    success_count = total_tests - failures - errors
    success_rate = (success_count / total_tests * 100) if total_tests > 0 else 0
    
    results = {
        'suite_name': suite_name,
        'total_tests': total_tests,
        'successes': success_count,
        'failures': failures,
        'errors': errors,
        'success_rate': success_rate,
        'duration': duration,
        'failures_detail': result.failures,
        'errors_detail': result.errors
    }
    
    # Print summary
    print(f"\n{suite_name} Summary:")
    print(f"  Tests run: {total_tests}")
    print(f"  Successes: {success_count}")
    print(f"  Failures: {failures}")
    print(f"  Errors: {errors}")
    print(f"  Success rate: {success_rate:.1f}%")
    print(f"  Duration: {duration:.2f}s")
    
    return results


def generate_test_report(all_results: list) -> str:
    """Generate comprehensive test report"""
    report = []
    report.append("# Comprehensive Skill Development System Test Report")
    report.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("")
    
    # Overall summary
    total_tests = sum(r['total_tests'] for r in all_results)
    total_successes = sum(r['successes'] for r in all_results)
    total_failures = sum(r['failures'] for r in all_results)
    total_errors = sum(r['errors'] for r in all_results)
    overall_success_rate = (total_successes / total_tests * 100) if total_tests > 0 else 0
    total_duration = sum(r['duration'] for r in all_results)
    
    report.append("## Overall Summary")
    report.append(f"- **Total Tests:** {total_tests}")
    report.append(f"- **Successes:** {total_successes}")
    report.append(f"- **Failures:** {total_failures}")
    report.append(f"- **Errors:** {total_errors}")
    report.append(f"- **Overall Success Rate:** {overall_success_rate:.1f}%")
    report.append(f"- **Total Duration:** {total_duration:.2f}s")
    report.append("")
    
    # Individual suite results
    report.append("## Test Suite Results")
    for result in all_results:
        report.append(f"### {result['suite_name']}")
        report.append(f"- Tests: {result['total_tests']}")
        report.append(f"- Success Rate: {result['success_rate']:.1f}%")
        report.append(f"- Duration: {result['duration']:.2f}s")
        
        if result['failures'] > 0:
            report.append(f"- **Failures ({result['failures']}):**")
            for test, traceback in result['failures_detail']:
                failure_msg = traceback.split('AssertionError: ')[-1].split('\n')[0] if 'AssertionError:' in traceback else 'Unknown failure'
                report.append(f"  - {test}: {failure_msg}")
        
        if result['errors'] > 0:
            report.append(f"- **Errors ({result['errors']}):**")
            for test, traceback in result['errors_detail']:
                error_msg = traceback.split('\n')[-2] if traceback else 'Unknown error'
                report.append(f"  - {test}: {error_msg}")
        
        report.append("")
    
    # Feature coverage analysis
    report.append("## Feature Coverage Analysis")
    report.append("### Task 3.2.1.1 - Dynamic Skill Discovery System ‚úÖ")
    report.append("- Action-based skill discovery")
    report.append("- Context-aware probability calculations")
    report.append("- Environment-based skill identification")
    report.append("- Discovery integration with skill execution")
    report.append("")
    
    report.append("### Task 3.2.6.1 - Skill Calculation Optimization ‚úÖ")
    report.append("- Caching system for expensive calculations")
    report.append("- Batch processing for multiple operations")
    report.append("- Pre-computed lookup tables")
    report.append("- Performance metrics and monitoring")
    report.append("")
    
    report.append("### Task 3.2.7.2 - Comprehensive Test Suites ‚úÖ")
    report.append("- Unit tests for individual components")
    report.append("- Integration tests for system interaction")
    report.append("- Performance benchmarking")
    report.append("- Regression testing for stability")
    report.append("")
    
    # Quality metrics
    report.append("## Quality Metrics")
    
    if overall_success_rate >= 95:
        quality_status = "üü¢ Excellent"
    elif overall_success_rate >= 90:
        quality_status = "üü° Good"
    elif overall_success_rate >= 80:
        quality_status = "üü† Acceptable"
    else:
        quality_status = "üî¥ Needs Improvement"
    
    report.append(f"- **Test Quality:** {quality_status}")
    report.append(f"- **Code Coverage:** Comprehensive (all major components tested)")
    report.append(f"- **Performance:** {'‚úÖ Optimized' if total_duration < 30 else '‚ö†Ô∏è Review needed'}")
    report.append("")
    
    # Recommendations
    report.append("## Recommendations")
    if total_failures > 0:
        report.append("- üîß Address failing test cases before production deployment")
    if total_errors > 0:
        report.append("- üêõ Fix error cases to improve system stability")
    if overall_success_rate < 95:
        report.append("- üéØ Aim for 95%+ success rate for production readiness")
    if total_duration > 30:
        report.append("- ‚ö° Consider further performance optimizations")
    
    if total_failures == 0 and total_errors == 0 and overall_success_rate >= 95:
        report.append("- ‚úÖ System is ready for production deployment")
        report.append("- üöÄ All quality gates passed successfully")
    
    return '\n'.join(report)


def main():
    """Main test runner"""
    print("Enhanced Skill Development System - Comprehensive Test Suite")
    print("Tasks 3.2.1.1, 3.2.6.1, and 3.2.7.2")
    print(f"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    all_results = []
    
    try:
        # Task 3.2.1.1: Dynamic Skill Discovery Tests
        discovery_results = run_test_suite(
            "Task 3.2.1.1 - Dynamic Skill Discovery System",
            [TestDynamicSkillDiscovery, TestSkillDiscoveryPerformance],
            verbose=True
        )
        all_results.append(discovery_results)
        
        # Task 3.2.6.1: Skill Optimization Tests
        optimization_results = run_test_suite(
            "Task 3.2.6.1 - Skill Calculation Optimization",
            [TestSkillOptimization, TestSkillOptimizationPerformance],
            verbose=True
        )
        all_results.append(optimization_results)
        
        # Task 3.2.7.2: Integration and Regression Tests
        integration_results = run_test_suite(
            "Task 3.2.7.2 - Integration & Regression Tests",
            [TestSkillSystemIntegration, TestSkillSystemRegressionTests],
            verbose=True
        )
        all_results.append(integration_results)
        
    except Exception as e:
        print(f"\n‚ùå Critical error during test execution: {e}")
        return 1
    
    # Generate comprehensive report
    print("\n" + "="*60)
    print("GENERATING COMPREHENSIVE TEST REPORT")
    print("="*60)
    
    report = generate_test_report(all_results)
    
    # Save report to file
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_filename = f"skill_system_test_report_{timestamp}.md"
    
    try:
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"\nüìÑ Comprehensive test report saved to: {report_filename}")
    except Exception as e:
        print(f"\n‚ö†Ô∏è  Could not save report to file: {e}")
    
    # Print report to console
    print("\n" + report)
    
    # Determine exit code
    total_failures = sum(r['failures'] for r in all_results)
    total_errors = sum(r['errors'] for r in all_results)
    
    if total_failures == 0 and total_errors == 0:
        print(f"\nüéâ ALL TESTS PASSED! System ready for deployment.")
        return 0
    else:
        print(f"\n‚ö†Ô∏è  Some tests failed. Review results before deployment.")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)